 The HuggingFace Datasets Library, a quick overview. The HuggingFace Datasets Library is a library that provides an API to quickly download many public datasets and pre-process them. In this video, we'll explore how to do that. The download part is easy with the loadDataSet function. You can directly download and cache a dataset from its identifier on the dataset hub. Here, we fetch the ARM RPC dataset from the glue benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the loadDataSet function is a dataset dict, which is a sort of dictionary containing each split of a dataset. You can access each split by indexing with its name. This split is then an instance of the dataset class with columns, here, sentence1, sentence2, label, an IDX, and rows. You can access a given element by its index. The amazing thing about the HuggingFace Datasets Library is that everything is saved to disk using Apache RU, which means that even if your dataset is huge, you won't get out of RAM. Only the elements you request are loaded in memory. Accessing a slice of your dataset is as easy as one element. The result is then a dictionary with list of values for each case, here, the list of labels, the list of first sentences, and the list of second sentences. The features attribute of a dataset gives us more information about its columns. In particular, we can see here it gives us the correspondence between the integers and names for the labels. 0 stands for not equivalent and 1 for equivalent. To process all the elements of our dataset, we need to tokenize them. Have a look at the video of prepossessed sentence pairs for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than its length, truncating inputs that are longer. We put all of this in the tokenize function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up prepossessing and take advantage of the fact our tokenizer is backed by Rust, thanks to the HuggingFace tokenizers library, we can process several elements at the same times in our tokenize function using the batch-equal-true argument. Since the tokenizer can handle list of first sentences, list of second sentences, the tokenize function does not need to change for this. You can also use prepossessing with the map method, check out its documentation, link below. Once this is done, we are almost ready for training. We just remove the columns we don't need anymore with the removeColons method, rename label to labels, set the models from the HuggingFace Transformers library expectVat, and set the output format to a desired packet, dodge, tensorflow, on and by. If needed, we can also generate a short sample of a data set using the select method.